---
title: "Who is using Wikidata Query Service and how are they using it?"
author: "Mikhail Popov"
date: "October 5, 2015"
output:
  html_document:
    fig_caption: no
    keep_md: yes
    toc: yes
  pdf_document:
    fig_caption: yes
    toc: yes
---

```{r setup, include = FALSE}
library(ggplot2)
library(magrittr)
library(knitr)
library(scales)
library(mclust)
library(gridExtra)
import::from(dplyr, group_by, select, summarize,
             right_join, left_join, ungroup, mutate,
             keep_where = filter)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
load('data/Queries_2015-10-05.RData')
```

```{r utils, include = FALSE}
top_n <- function(x, n = 10) {
  y <- sort(table(x), decreasing = TRUE)
  head(names(y), n)
}
"%notin%" <- function(x, y) !(x %in% y)
smart_trim <- function(x, width) {
  trimmed <- paste0(strtrim(x, width), '...')
  ifelse(sapply(x, nchar) > width, trimmed, x)
}
```

## Introduction

The Wikidata Query Service (WDQS) is designed to let users run queries on the data contained in Wikidata. The service uses SPARQL as the query language. *SPARQL Protocol and RDF Query Language* (SPARQL) allows users to write queries against what can loosely be called "key-value" data or, more specifically, data that follows the RDF specification of the W3C. The entire database is thus a set of "subject-predicate-object" triples.

## Statistics

```{r median_queries_per_user}
wdqs_queries %>%
  group_by(date, user_id) %>%
  summarize(`queries per user` = n()) %>%
  group_by(date) %>%
  summarize(`median queries per user` = median(`queries per user`),
            `25%` = quantile(`queries per user`, 0.25),
            `75%` = quantile(`queries per user`, 0.75)) %>%
  ggplot(data = .) +
  geom_ribbon(aes(x = date, ymax = `75%`, ymin = `25%`),
              alpha = 0.3) +
  geom_vline(xintercept = as.numeric(as.Date("2015-09-07")),
             linetype = "dashed", color = "purple") +
  geom_text(x = as.numeric(as.Date("2015-09-14")), y = 60,
            label = "Announcement", color = "purple") +
  geom_line(aes(x = date, y = `median queries per user`)) +
  wmf::theme_fivethirtynine()
```

The lower and upper bounds represent the first and third quartiles (25% and 75%). Here we can see that the number of queries per user has stabilized a lot after the announcement.

```{r countries, eval = FALSE}
summarize(group_by(users, countries), n = n())$countries %>%
  sort %>% paste0(collapse = ', ') %>% print
```

WDQS users are a very geographically diverse bunch! In fact, 73 different countries were represented between August 23<sup>rd</sup> and October 4<sup>th</sup>: Algeria, Angola, Argentina, Armenia, Australia, Austria, Azerbaijan, Belarus, Belgium, Brazil, Bulgaria, Cambodia, Canada, Chile, China, Colombia, Croatia, Czech Republic, Denmark, Ecuador, Egypt, Estonia, Finland, France, Germany, Ghana, Greece, Guadeloupe, Hungary, India, Indonesia, Iran, Ireland, Israel, Italy, Japan, Latvia, Luxembourg, Malaysia, Mali, Malta, Martinique, Mexico, Montenegro, Nepal, Netherlands, New Zealand, Norway, Poland, Portugal, Qatar, Republic of Korea, Romania, Russia, Saudi Arabia, Serbia, Singapore, Slovak Republic, Slovenia, South Africa, Spain, Sri Lanka, Sweden, Switzerland, Taiwan, Thailand, Turkey, Ukraine, United Kingdom, United States, Uruguay, Venezuela, and Vietnam.

```{r top_10_countries, width = 4, height = 8}
users %>%
  mutate(Country = ifelse(countries %in% top_n(countries),
                          countries, 'Other')) %>%
  group_by(Country) %>%
  summarize(Users = n()) %>%
  ggplot(data = ., aes(y = Users, x = Country)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Top 10 countries by number of WDQS users") +
  wmf::theme_fivethirtynine()
```

U.S., U.K., Germany, and France are the top-represented countries, with U.S. leading the pack.

```{r top_10_browsers, width = 8, height = 4}
users %>%
  mutate(Browser = ifelse(browser %in% top_n(browser, 5),
                          browser, 'Other')) %>%
  group_by(Browser) %>%
  summarize(Users = n()) %>%
  ggplot(data = ., aes(y = Users, x = Browser)) +
  geom_bar(stat = "identity") +
  # coord_flip() +
  ggtitle("Top 5 browsers by number of WDQS users") +
  wmf::theme_fivethirtynine()
```

Chrome and Firefox are, unsurprisingly, WDQS users' preferred browsers.

```{r top_10_oses, width = 4, height = 8}
users %>%
  mutate(`Operating System` = ifelse(os %in% top_n(os, 10),
                                     os, 'Other')) %>%
  group_by(`Operating System`) %>%
  summarize(Users = n()) %>%
  ggplot(data = ., aes(y = Users, x = `Operating System`)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ggtitle("Top 5 OSes by number of WDQS users") +
  wmf::theme_fivethirtynine()
```

Windows 7 and Mac OS X users are by far the most popular operating systems among WDQS users.

![](figures/queries.png)

Total queries over time and how many were the sample queries we provided for demonstration.

```{r queries_from_countries_over_time, width = 8, height = 16}
wdqs_queries %>%
  left_join(users, by = "user_id") %>%
  keep_where(countries != 'Unknown') %>%
  mutate(Country = ifelse(countries %in% top_n(countries, 6),
                          countries, 'Other'),
         Day = as.POSIXct(round(timestamp, "days"))) %>%
  keep_where(Country != 'Other') %>%
  group_by(Day, Country) %>%
  summarize(Queries = n()) %>%
  ggplot(data = ., aes(x = Day, y = Queries)) +
  facet_grid(Country ~ .) +
  scale_x_datetime(labels = date_format("%a %m/%d")) +
  geom_vline(xintercept = as.numeric(lubridate::ymd("2015-09-07")),
             linetype = "dashed", color = "purple") +
  # geom_smooth(method = "loess", se = FALSE) +
  geom_line() +
  theme_bw()
```

Varying patterns of WDQS usage by country (top 5 countries, over time). Purple dashes mark the public announcement.

```{r users_from_countries_over_time, width = 8, height = 16}
wdqs_queries %>%
  left_join(users, by = "user_id") %>%
  keep_where(countries != 'Unknown') %>%
  mutate(Country = ifelse(countries %in% top_n(countries, 6),
                          countries, 'Other'),
         Day = as.POSIXct(round(timestamp, "days"))) %>%
  keep_where(Country != 'Other') %>%
  group_by(Day, Country) %>%
  summarize(Users = (function(x) { length(unique(x)) })(user_id)) %>%
  ggplot(data = ., aes(x = Day, y = Users)) +
  facet_grid(Country ~ .) +
  scale_x_datetime(labels = date_format("%a %m/%d")) +
  geom_vline(xintercept = as.numeric(lubridate::ymd("2015-09-07")),
             linetype = "dashed", color = "purple") +
  # geom_smooth(method = "loess", se = FALSE) +
  geom_line() +
  theme_bw()
```

Varying patterns of WDQS unique users by country (top 5 countries, over time). Purple dashes mark the public announcement. What is very interesting is that South Korea is a top 5 country in usage but with barely any users.

### Who are our most active users?

**Note**: "user_id" is an anonymous identification marker created post-hoc from IP address and user agent. It is used for linking queries.

#### Top 20 users by total queries

```{r most_active_overall, as.is = TRUE}
most_active_overall <- wdqs_queries %>%
  group_by(user_id) %>%
  summarize(`total queries` = n()) %>%
  dplyr::top_n(20, `total queries`) %>%
  dplyr::arrange(desc(`total queries`)) %>%
  left_join(users, by = "user_id") %>%
  select(-c(browser, device)) %>%
  dplyr::rename(country = countries)
knitr::kable(most_active_overall)
```

#### Top 20 users by daily service usage

```{r most_active_daily, as.is = TRUE}
most_active_daily <- wdqs_queries %>%
  group_by(user_id, date) %>%
  summarize(`queries per day` = n()) %>%
  group_by(user_id) %>%
  summarize(`median queries per day` = median(`queries per day`)) %>%
  dplyr::top_n(20, `median queries per day`) %>%
  dplyr::arrange(desc(`median queries per day`)) %>%
  left_join(users, by = "user_id") %>%
  select(-c(browser, device)) %>%
  dplyr::rename(country = countries)
knitr::kable(most_active_daily)
```

#### Users who made it to both lists

```{r most_active_both, as.is = TRUE}
dplyr::inner_join(select(most_active_overall, c(user_id, `total queries`)),
                  select(most_active_daily, c(user_id, `median queries per day`)),
                  by = "user_id") %>%
  left_join(users) %>%
  dplyr::arrange(desc(`median queries per day`)) %>%
  select(-c(browser, device)) %>%
  dplyr::rename(country = countries) %>%
  knitr::kable()
```

```{r most_active_cleanup}
rm(most_active_overall, most_active_daily)
```

### Referers

```{r referers, as.is = TRUE}
wdqs_queries %>%
  mutate(referer = smart_trim(referer, 35)) %>%
  group_by(referer, user_id) %>%
  summarize(`queries by user` = n()) %>%
  group_by(referer) %>%
  summarize(users = n()) %>%
  dplyr::top_n(10, users) %>%
  dplyr::arrange(desc(users)) %>%
  knitr::kable()
```

## Queries

### Query lengths

```{r nchar_hist, width = 10, height = 5}
wdqs_queries %<>% mutate(length = nchar(query))
ggplot(data = keep_where(wdqs_queries, sample == "no"),
       aes(x = length)) +
  geom_histogram(binwidth = 100, alpha = 0.8) +
  xlab("Number of characters in query") +
  ggtitle("Distribution of query lengths:") +
  wmf::theme_fivethirtynine()
```

We can see multiple modes in the distribution of query lengths, which suggests that the distribution is a mixture of several distributions. The next step is to use a clustering algorithm to separate the distributions out into distinct groups. For this task, we chose a model-based clustering algorithm.

We performed model-based clustering on the log10-transformed character counts of queries that were not sample queries we provided. (Model-based clustering relies on Gaussian mixture models, so the log10 transformation was employed to correct for the right-skewness and make the data Normal.)

```{r nchar_clust, cache = TRUE}
# Use model-based clustering on the log10 transformed character counts:
set.seed(0)
wdqs_queries_nonexample <- wdqs_queries %>%
  keep_where(sample == "no") %>%
  mutate(log10length = log10(length))
clust <- Mclust(wdqs_queries_nonexample$log10length, 4)
wdqs_queries_nonexample$cluster <- factor(LETTERS[clust$classification])

# Merge into overall dataset:
wdqs_queries <- wdqs_queries_nonexample %>%
  select(query_id, cluster) %>%
  left_join(wdqs_queries, ., by = "query_id")

# For calculating f(x):
x = seq(min(wdqs_queries_nonexample$log10length),
        max(wdqs_queries_nonexample$log10length),
        length.out = 1000)

# Calculate the f(x) using the mean and sd of that cluster:
densities <- sapply(1:length(clust$parameters$pro),
                    function(i) {
                      dnorm(x,
                            mean = clust$parameters$mean[i],
                            sd = sqrt(clust$parameters$variance$sigmasq[i]))
                    }) %>%
  as.data.frame() %>%
  { colnames(.) <- LETTERS[1:ncol(.)]; . } %>%
  cbind(n = x, .) %>%
  # Convert wide to long:
  tidyr::gather("cluster", "density", -1) %>%
  # Normalize:
  group_by(cluster) %>%
  mutate(density = density/max(density)) %>%
  ungroup

# Adjust densities for visualization:
for (i in 1:length(clust$parameters$pro) ) {
  densities$density[densities$cluster == LETTERS[i]] %<>%
    { . * clust$parameters$pro[i] }
}; rm(i)

# This will be used to add cluster means (on the back-transformed scale) to the plot:
annotations <- data.frame(x = unname(round(10^clust$parameters$mean, 1)),
                          y = rep(0.3, length(clust$parameters$pro)),
                          cluster = factor(levels(wdqs_queries_nonexample$cluster)))

# Plot the log-normal densities:
ggplot(data = densities) +
  geom_line(aes(x = 10^n, y = density, color = cluster),
            size = 1.1) +
  xlab("Number of characters in query") +
  ggtitle("Four distinct categories of query lengths") +
  scale_color_discrete(name = "Group") +
  geom_vline(data = annotations, linetype = "dashed",
             aes(xintercept = x, color = cluster)) +
  geom_text(data = annotations, aes(label = x, x = x, y = y)) +
  wmf::theme_fivethirtynine() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.line.y = element_blank())

# Clean up:
rm(x, densities, annotations, wdqs_queries_nonexample)
```

The most optimal model was a 4-component univariate mixture with unequal variances. The centers for the 4 clusters (on the raw scale) are: 89, 388, 621, and 1177 characters.

### Example queries

#### Shortest queries

```{r example_query_shortest}
wdqs_queries %>%
  keep_where(sample == "no") %>%
  dplyr::arrange(length) %>%
  select(query) %>%
  unique() %>%
  head(25) %>%
  unlist %>%
  unname %>%
  matrix(nrow = 5, ncol = 5) %>%
  as.data.frame() %>%
  knitr::kable(col.names = paste("column", 1:5))
```

#### Longest query

```{r longest_query, eval = FALSE}
wdqs_queries %>%
  keep_where(sample == "no") %>%
  select(query, length) %>%
  unique() %>%
  dplyr::top_n(1, length) %>%
  select(query) %>%
  unlist %>%
  cat
```

#### Examples of Category "A" queries:

#### Examples of Category "B" queries:

#### Examples of Category "C" queries:

#### Examples of Category "D" queries:

## References

- [*Wikidata query service* on MediaWiki](https://www.mediawiki.org/wiki/Wikidata_query_service) and [WDQS User Manual](https://www.mediawiki.org/wiki/Wikidata_query_service/User_Manual)
- [*SPARQL* on Wikipedia](https://en.wikipedia.org/wiki/SPARQL)
- [**mclust**](http://www.stat.washington.edu/mclust/): Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation by Fraley, C. and [Raftery, A.](https://en.wikipedia.org/wiki/Adrian_Raftery)
